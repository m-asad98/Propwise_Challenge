{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "493bfcf2-9215-43b7-bd81-b7c58bd2799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, date_format, round, when, expr\n",
    "#from google.colab import drive\n",
    "from datetime import datetime\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "spark=SparkSession.builder.appName('Propwise').config(\"spark.jars\", r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\postgresql-42.7.6.jar\").getOrCreate()\n",
    "\n",
    "#file_path = '/content/drive/MyDrive/Propwise Challenge/transactions-2025-06-07.csv'\n",
    "file_path = r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\transactions-2025-06-07.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "#df.printSchema()\n",
    "\n",
    "\"\"\"Cleaning and transforming data\"\"\"\n",
    "\n",
    "def transform(df):\n",
    "\n",
    "  ### Remove null and duplicate records\n",
    "  cleaned_df = df.na.drop()\n",
    "  cleaned_df = cleaned_df.dropDuplicates()\n",
    "  #df = df.dropna()\n",
    "  #df = df.drop_duplicates()\n",
    "\n",
    "  ### Standardize column names\n",
    "  for oldCol in cleaned_df.columns:\n",
    "    cleaned_df = cleaned_df.withColumnRenamed(oldCol, oldCol.lower().replace(' ','_'))\n",
    "  #df.columns = [x.lower().replace(' ','_') for x in df.columns]\n",
    "\n",
    "  ### Parse dates\n",
    "\n",
    "  # Convert instance_date column to type timestamp\n",
    "  cleaned_df = cleaned_df.withColumn('instance_date_timestamp', to_timestamp(col('instance_date'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "  # Place the date and time in their own new columns\n",
    "  cleaned_df = cleaned_df.withColumn('date_col', date_format(col('instance_date_timestamp'), 'yyyy-MM-dd'))\n",
    "  cleaned_df = cleaned_df.withColumn('time_col', date_format(col('instance_date_timestamp'), 'HH:mm:ss'))\n",
    "\n",
    "  # Drop the instance_date column\n",
    "  #cleaned_df = cleaned_df.drop('instance_date')\n",
    "\n",
    "  #df = df.withColumn('date', to_timestamp(col('date'), 'MM/dd/yyyy'))\n",
    "  #if df.columns.str.contains('date', case= False).any():\n",
    "   # df.columns = pd.to_datetime(df.columns)\n",
    "\n",
    "  ### Parse numeric columns\n",
    "  for column in [\"trans_value\", \"procedure_area\", \"actual_area\"]:\n",
    "    cleaned_df = cleaned_df.withColumn(column, round(col(column).cast('double'),2))\n",
    "\n",
    "  ### New columns (price_per_sqm, price_per_room, budget_tier)\n",
    "  cleaned_df = cleaned_df.withColumn('price_per_sqm', round(col('trans_value')/col('procedure_area'),2))\n",
    "  #cleaned_df = cleaned_df.withColumn('property_age', 2025 - col('date_col'))\n",
    "  cleaned_df = cleaned_df.withColumn('price_per_room', round(col(\"trans_value\") / when(col(\"rooms_en\") == \"Studio\", 1).otherwise(expr(\"try_cast(substr(rooms_en, 1, 1) as int)\")), 2))\n",
    "  cleaned_df = cleaned_df.withColumn('budget_tier', when(col(\"trans_value\")<=600000, \"Low Budget\").when((col(\"trans_value\")>600000) & (col(\"trans_value\")<=2500000), \"Medium Budget\").otherwise(\"High Budget\"))\n",
    "\n",
    "\n",
    "\n",
    "  return cleaned_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad6840d0-55e7-4bda-95a9-95a3e9cab6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+--------------------+-------------+---------------+-----------+--------------------+------------+---------------+-----------+--------------+-----------+--------+-------+--------------------+--------------------+--------------------+-----------+------------+--------------------+--------------------+-----------------------+----------+--------+-------------+--------------+-------------+\n",
      "|transaction_number|      instance_date|group_en|        procedure_en|is_offplan_en|is_free_hold_en|   usage_en|             area_en|prop_type_en|prop_sb_type_en|trans_value|procedure_area|actual_area|rooms_en|parking|    nearest_metro_en|     nearest_mall_en| nearest_landmark_en|total_buyer|total_seller|   master_project_en|          project_en|instance_date_timestamp|  date_col|time_col|price_per_sqm|price_per_room|  budget_tier|\n",
      "+------------------+-------------------+--------+--------------------+-------------+---------------+-----------+--------------------+------------+---------------+-----------+--------------+-----------+--------+-------+--------------------+--------------------+--------------------+-----------+------------+--------------------+--------------------+-----------------------+----------+--------+-------------+--------------+-------------+\n",
      "|     11-18140-2025|2025-05-01 11:54:20|   Sales|                Sale|        Ready|      Free Hold|Residential|           AL FURJAN|        Unit|           Flat|   580000.0|          31.8|       31.8|  Studio|   B-45|ENERGY Metro Station|  Ibn-e-Battuta Mall|      Expo 2020 Site|          0|           0|Jebel Ali Village...|         AZIZI AMBER|    2025-05-01 11:54:20|2025-05-01|11:54:20|     18238.99|      580000.0|   Low Budget|\n",
      "|    102-34635-2025|2025-05-05 07:13:17|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|  1059900.0|          62.5|       62.5|   1 B/R|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|    Maison Elysee II|    2025-05-05 07:13:17|2025-05-05|07:13:17|      16958.4|     1059900.0|Medium Budget|\n",
      "|    102-45636-2025|2025-05-28 11:16:45|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   779900.0|         36.68|      36.68|  Studio|      1|Nakheel Metro Sta...|         Marina Mall|Sports City Swimm...|          0|           0|    Maison Elysee II|MAISON ELYSEE III...|    2025-05-28 11:16:45|2025-05-28|11:16:45|     21262.27|      779900.0|Medium Budget|\n",
      "|    102-47143-2025|2025-06-02 14:17:36|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   715671.0|         36.95|      36.95|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|    Maison Elysee II|    2025-06-02 14:17:36|2025-06-02|14:17:36|     19368.63|      715671.0|Medium Budget|\n",
      "|    102-44389-2025|2025-05-23 19:44:15|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   679900.0|         35.37|      35.37|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|    Maison Elysee II|    2025-05-23 19:44:15|2025-05-23|19:44:15|      19222.5|      679900.0|Medium Budget|\n",
      "|    102-34650-2025|2025-05-05 10:35:42|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|  1050000.0|         83.47|      83.47|   2 B/R|      2| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|    Maison Elysee II|    2025-05-05 10:35:42|2025-05-05|10:35:42|     12579.37|      525000.0|Medium Budget|\n",
      "|    102-40343-2025|2025-05-12 23:23:51|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|  1035405.0|         63.84|      63.84|   1 B/R|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|    Maison Elysee II|    2025-05-12 23:23:51|2025-05-12|23:23:51|     16218.75|     1035405.0|Medium Budget|\n",
      "|    102-42148-2025|2025-05-19 09:14:21|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|  1089900.0|          58.4|       58.4|   1 B/R|      1|Nakheel Metro Sta...|         Marina Mall|Sports City Swimm...|          0|           0|    Maison Elysee II|MAISON ELYSEE III...|    2025-05-19 09:14:21|2025-05-19|09:14:21|     18662.67|     1089900.0|Medium Budget|\n",
      "|    102-41062-2025|2025-05-14 21:21:45|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   557000.0|         35.31|      35.31|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|    Maison Elysee II|    2025-05-14 21:21:45|2025-05-14|21:21:45|     15774.57|      557000.0|   Low Budget|\n",
      "|    102-43924-2025|2025-05-23 07:07:36|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   764900.0|         36.68|      36.68|  Studio|      1|Nakheel Metro Sta...|         Marina Mall|Sports City Swimm...|          0|           0|    Maison Elysee II|MAISON ELYSEE III...|    2025-05-23 07:07:36|2025-05-23|07:07:36|     20853.33|      764900.0|Medium Budget|\n",
      "|    102-39812-2025|2025-05-12 00:00:58|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   749900.0|         37.66|      37.66|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|     Maison Elysee I|    2025-05-12 00:00:58|2025-05-12|00:00:58|     19912.37|      749900.0|Medium Budget|\n",
      "|    102-45892-2025|2025-05-29 00:09:11|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|  1084900.0|         57.69|      57.69|   1 B/R|      1|Nakheel Metro Sta...|         Marina Mall|Sports City Swimm...|          0|           0|    Maison Elysee II|MAISON ELYSEE III...|    2025-05-29 00:09:11|2025-05-29|00:09:11|     18805.69|     1084900.0|Medium Budget|\n",
      "|    102-44486-2025|2025-05-26 08:56:43|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   682900.0|         37.16|      37.16|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|     Maison Elysee I|    2025-05-26 08:56:43|2025-05-26|08:56:43|     18377.29|      682900.0|Medium Budget|\n",
      "|    102-40347-2025|2025-05-12 23:24:44|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|  1000000.0|         58.74|      58.74|   1 B/R|      1|Nakheel Metro Sta...|         Marina Mall|Sports City Swimm...|          0|           0|    Maison Elysee II|MAISON ELYSEE III...|    2025-05-12 23:24:44|2025-05-12|23:24:44|     17024.17|     1000000.0|Medium Budget|\n",
      "|    102-39808-2025|2025-05-11 23:39:04|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   674405.0|         38.18|      38.18|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|     Maison Elysee I|    2025-05-11 23:39:04|2025-05-11|23:39:04|     17663.83|      674405.0|Medium Budget|\n",
      "|    102-41415-2025|2025-05-15 13:49:18|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   780000.0|         63.84|      63.84|   1 B/R|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|     Maison Elysee I|    2025-05-15 13:49:18|2025-05-15|13:49:18|     12218.05|      780000.0|Medium Budget|\n",
      "|    102-44484-2025|2025-05-26 08:59:53|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   699900.0|         36.53|      36.53|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|     Maison Elysee I|    2025-05-26 08:59:53|2025-05-26|08:59:53|     19159.59|      699900.0|Medium Budget|\n",
      "|    102-45889-2025|2025-05-29 00:17:42|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   692900.0|         37.06|      37.06|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|    Maison Elysee II|    2025-05-29 00:17:42|2025-05-29|00:17:42|     18696.71|      692900.0|Medium Budget|\n",
      "|    102-39361-2025|2025-05-09 05:21:22|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|  1052353.0|          58.4|       58.4|   1 B/R|      1|Nakheel Metro Sta...|         Marina Mall|Sports City Swimm...|          0|           0|    Maison Elysee II|MAISON ELYSEE III...|    2025-05-09 05:21:22|2025-05-09|05:21:22|     18019.74|     1052353.0|Medium Budget|\n",
      "|    102-44036-2025|2025-05-23 09:51:29|   Sales|Sell - Pre regist...|     Off-Plan|      Free Hold|Residential|JUMEIRAH VILLAGE ...|        Unit|           Flat|   494900.0|         35.37|      35.37|  Studio|      1| Dubai Internet City|Mall of the Emirates|Sports City Swimm...|          0|           0|      Elysee Heights|     Maison Elysee I|    2025-05-23 09:51:29|2025-05-23|09:51:29|     13992.08|      494900.0|   Low Budget|\n",
      "+------------------+-------------------+--------+--------------------+-------------+---------------+-----------+--------------------+------------+---------------+-----------+--------------+-----------+--------+-------+--------------------+--------------------+--------------------+-----------+------------+--------------------+--------------------+-----------------------+----------+--------+-------------+--------------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = transform(df)\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9453f1c9-288f-40ef-bf05-faf97f20dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(cleaned_df, table_name):\n",
    "  # PostgreSQL info\n",
    "  postgresql_url = r\"jdbc:postgresql://localhost:5432/UAE_Real_Estate\"\n",
    "  properties = {\n",
    "      \"user\" : \"postgres\",\n",
    "      \"password\" : \"123\",\n",
    "      \"driver\" : \"org.postgresql.Driver\"\n",
    "  }\n",
    "\n",
    "  # Write DataFrame to PostgreSQL table\n",
    "  cleaned_df.write.jdbc(url=postgresql_url, table=table_name, mode=\"overwrite\", properties=properties)\n",
    "  \n",
    "\n",
    "cleaned_df = transform(df)\n",
    "load(cleaned_df, \"dubai_real_estate_transactions\")\n",
    "\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf4748cc-90c0-4482-b64f-ba16e5b20e1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2358.csv.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\npy4j.ClientServerConnection.run(ClientServerConnection.java:108)\njava.base/java.lang.Thread.run(Thread.java:840)\n\nAnd it was stopped at:\n\norg.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:552)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\npy4j.ClientServerConnection.run(ClientServerConnection.java:108)\njava.base/java.lang.Thread.run(Thread.java:840)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\r\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2872)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:63)\r\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.scala:17)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:60)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:174)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:173)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:668)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:603)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:558)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:284)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:279)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\r\n\t\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2872)\r\n\t\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:63)\r\n\t\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.scala:17)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:60)\r\n\t\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:174)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:173)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:668)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:603)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:558)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:284)\r\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:279)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:402)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAsad\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPropwise Challenge\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcleaned_transactions_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcleaned_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\readwriter.py:2146\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   2129\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   2130\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2144\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   2145\u001b[0m )\n\u001b[1;32m-> 2146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\spark_env\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\.conda\\envs\\spark_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\.conda\\envs\\spark_env\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2358.csv.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\npy4j.ClientServerConnection.run(ClientServerConnection.java:108)\njava.base/java.lang.Thread.run(Thread.java:840)\n\nAnd it was stopped at:\n\norg.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:552)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\npy4j.ClientServerConnection.run(ClientServerConnection.java:108)\njava.base/java.lang.Thread.run(Thread.java:840)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\r\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2872)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:63)\r\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.scala:17)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:60)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:174)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:173)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:668)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:603)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:558)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:284)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:279)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\r\n\t\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2872)\r\n\t\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:63)\r\n\t\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.scala:17)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:60)\r\n\t\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:174)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:173)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:668)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:603)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createNonResultQueryStages$2(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2141)\r\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createNonResultQueryStages(AdaptiveSparkPlanExec.scala:643)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:558)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:284)\r\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:279)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:402)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\cleaned_transactions_output\"\n",
    "cleaned_df.write.option(\"header\", True).mode(\"overwrite\").csv(output_csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6a498-4f1d-40ea-89bf-3bf07e7ebe83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

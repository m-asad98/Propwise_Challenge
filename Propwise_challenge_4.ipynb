{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb0ccb87-bd36-477d-a803-85fbc45aa5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, date_format, round, when, expr, lit\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3842c07-f07f-410f-a277-27df0daa3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize a SparkSession named Propwise with a jar config. A different port has also been defined to avoid initialization failures. Change the path for the .jar file to point to the file in your system.\n",
    "spark=SparkSession.builder.appName('Propwise').config(\"spark.jars\", r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\postgresql-42.7.6.jar\").config(\"spark.driver.port\", \"4041\").config(\"spark.ui.port\", \"4041\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4aba34b3-b234-47f2-81ac-55feba185e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The input file is being read. Change the file path to point to your own data file.\n",
    "#file_path = '/content/drive/MyDrive/Propwise Challenge/transactions-2025-06-07.csv'\n",
    "file_path = r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\transactions-2025-06-11.csv\"\n",
    "\n",
    "#The options for read.csv have been defined because there was malformed data where a field had a line break and went to the next, causing parsing issues. It is read correctly with enabling multiline and defining quote and escape characters\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, multiLine= True, quote='\"', escape='\"')\n",
    "#df_rows = df.count()\n",
    "#df.printSchema()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "493bfcf2-9215-43b7-bd81-b7c58bd2799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defining a function to clean the data of any null and duplicate records, make the column names more consistent and to correctly parse date and numeric values\"\"\"\n",
    "\n",
    "def transform(df):\n",
    "\n",
    "  ### Standardize column names by iterating through all the columns, converting them to lower case and replacing spaces with underscores(_)\n",
    "  for oldCol in df.columns:\n",
    "    df = df.withColumnRenamed(oldCol, oldCol.lower().replace(' ','_'))\n",
    "\n",
    "  ### Remove null values and any duplicate rows that may be present in the data\n",
    "  cleaned_df = df.na.drop(subset=[\"transaction_number\"])\n",
    "  cleaned_df = cleaned_df.dropDuplicates()\n",
    "\n",
    "  ### Parse date values that were being read as string by converting to timestamp data type and then splitting the date and time in their own separate columns\n",
    "\n",
    "  # Convert instance_date column to type timestamp as it was being read as string due to format discrepancies\n",
    "  cleaned_df = cleaned_df.withColumn('instance_date_timestamp', expr(\"try_to_timestamp(instance_date, 'yyyy-MM-dd HH:mm:ss')\"))\n",
    "\n",
    "  # Place the date and time in their own new columns\n",
    "  cleaned_df = cleaned_df.withColumn('date_col', date_format(col('instance_date_timestamp'), 'yyyy-MM-dd'))\n",
    "  cleaned_df = cleaned_df.withColumn('time_col', date_format(col('instance_date_timestamp'), 'HH:mm:ss'))\n",
    "\n",
    "  ### Parse numeric columns by ensuring they are of data type 'double' and rounding them to 2 decimal places\n",
    "  for column in [\"trans_value\", \"procedure_area\", \"actual_area\"]:\n",
    "    cleaned_df = cleaned_df.withColumn(column, round(col(column).cast('double'),2))\n",
    "\n",
    "  ### New columns (price_per_sqm, price_per_room, budget_tier, has_parking)\n",
    "  # Calculating price per sqm by dividing the trans_value by procedure_area and rounding to 2\n",
    "  cleaned_df = cleaned_df.withColumn('price_per_sqm', round(col('trans_value')/col('procedure_area'),2))\n",
    "  # Calculating price per sqm by dividing the trans_value by rooms_en (considering Studio to be 1 room) and rounding to 2\n",
    "  cleaned_df = cleaned_df.withColumn('price_per_room', round(col(\"trans_value\") / when(col(\"rooms_en\") == \"Studio\", 1).otherwise(expr(\"try_cast(substr(rooms_en, 1, 1) as int)\")), 2))\n",
    "  # Getting the budget tier (Low, Medium or High budget) by defining ranges for each for the trans_value field\n",
    "  cleaned_df = cleaned_df.withColumn('budget_tier', when(col(\"trans_value\")<=600000, \"Low Budget\").when((col(\"trans_value\")>600000) & (col(\"trans_value\")<=2500000), \"Medium Budget\").otherwise(\"High Budget\"))\n",
    "  # Creating a new field has_parking which is essentially a flag for whether a property has parking or not\n",
    "  cleaned_df = cleaned_df.withColumn(\"has_parking\", when(col(\"parking\").isNull(), \"NO\").otherwise(\"YES\"))\n",
    "  \n",
    "  return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9453f1c9-288f-40ef-bf05-faf97f20dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Funtion to load the cleaned and transformed data into a local PostgreSQL database \"\"\"\n",
    "\n",
    "def load(cleaned_df, table_name):\n",
    "  # PostgreSQL info for the local instance and database to load the data into. Replace with your own database url, username and password\n",
    "  postgresql_url = r\"jdbc:postgresql://localhost:5432/UAE_Real_Estate\"\n",
    "  properties = {\n",
    "      \"user\" : \"postgres\",\n",
    "      \"password\" : \"123\",\n",
    "      \"driver\" : \"org.postgresql.Driver\"\n",
    "  }\n",
    "\n",
    "  # Write DataFrame to PostgreSQL table\n",
    "  cleaned_df.write.jdbc(url=postgresql_url, table=table_name, mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb608668-b1ac-4ab1-8c5e-19e886310a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the transform and load functions. Replace the table name with the you need.\n",
    "cleaned_df = transform(df)\n",
    "load(cleaned_df, \"dubai_real_estate_transactions\")\n",
    "#cleaned_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdf6a498-4f1d-40ea-89bf-3bf07e7ebe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining a function to conduct analysis on the data that was transformed and loaded in the PostgreSQL database. The data is being read from the database table for analysis \"\"\"\n",
    "\n",
    "def analyze():\n",
    "\n",
    "    # Read the data from PostgreSQL to use for analysis\n",
    "    postgresql_url = r\"jdbc:postgresql://localhost:5432/UAE_Real_Estate\"\n",
    "    properties = {\n",
    "      \"user\" : \"postgres\",\n",
    "      \"password\" : \"123\",\n",
    "      \"driver\" : \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    transactions_data = spark.read.jdbc(url=postgresql_url, table=\"dubai_real_estate_transactions\", properties=properties)\n",
    "    \n",
    "    ### Get the average price per region by grouping the data by area_en field and calculating the average of tranaction values\n",
    "    avg_price_per_region = transactions_data.groupBy(\"area_en\").agg({\"trans_value\":\"avg\"}).withColumnRenamed(\"avg(trans_value)\", \"avg_price_per_region\").dropna(subset=[\"area_en\"])\n",
    "    #Convert the Spark dataframe to a pandas dataframe\n",
    "    avg_price_per_region_pd = avg_price_per_region.toPandas()\n",
    "    \n",
    "    #Convert the values for the transactgions to numeric data type in case any are read as strings\n",
    "    avg_price_per_region_pd[\"avg_price_per_region\"] = pd.to_numeric(avg_price_per_region_pd[\"avg_price_per_region\"], errors=\"coerce\").round(2)\n",
    "\n",
    "    # saving the output as a CSV. Change the path to your local one.\n",
    "    avg_price_per_region_pd.to_csv(r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\analytics_files\\avg_price_per_region.csv\", index=False)\n",
    "    \n",
    "    # Plot a bar graph for the average prices per region. Only top 20 regions are being plotted.\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.bar(avg_price_per_region_pd[\"area_en\"][:20], avg_price_per_region_pd[\"avg_price_per_region\"][:20], width=0.8)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.title(\"Average Price per Region\")\n",
    "    plt.ylabel(\"Price (AED)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\analytics_files\\avg_price_per_region.png\") #Change the path to save the plot to your own system\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    ### Get the transactions per month by first extracting the year and month from the transaction date contained in the date_col field into separate columns and then grouping the by year and month both while applying a count function to get the total number of records, hence the total number of transactions\n",
    "    from pyspark.sql.functions import month, year, count, concat_ws\n",
    "\n",
    "    month_year_cols = transactions_data.withColumn(\"month\", month(\"date_col\")).withColumn(\"year\", year(\"date_col\"))\n",
    "    trans_month = month_year_cols.groupBy(\"year\", \"month\").agg(count(\"*\").alias(\"transaction_count\"))\n",
    "    trans_month = trans_month.withColumn(\"month_label\", concat_ws(\"-\", col(\"year\"), col(\"month\")))\n",
    "\n",
    "    trans_month_pd = trans_month.select(\"month_label\", \"transaction_count\").orderBy(\"month_label\").toPandas()\n",
    "    \n",
    "    # Write the months and the count of their transactions to a CSV. Change the path to your local one.\n",
    "    trans_month_pd.to_csv(r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\analytics_files\\transactions_per_month.csv\", index=False)\n",
    "\n",
    "    # Plot a line graph for the transactions per month\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(trans_month_pd[\"month_label\"], trans_month_pd[\"transaction_count\"], marker=\"o\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"Number of Transactions per Month\")\n",
    "    plt.ylabel(\"Transactions\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\analytics_files\\transactions_per_month.png\") #Change the path to save the plot to your own system\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    ### Get the highest and lowest priced properties by ordering the data descending first and then ascending using the trans_value field and getting the first record. This will give us the highest value first and the lowest value when the data is sorted as ascending\n",
    "\n",
    "    # Highest price\n",
    "    highest_price = transactions_data.orderBy(\"trans_value\", ascending=False).limit(1)\n",
    "    # Lowest price\n",
    "    lowest_price = transactions_data.orderBy(\"trans_value\").limit(1)\n",
    "\n",
    "    # Combine and write to CSV\n",
    "    highest_and_lowest_price = highest_price.union(lowest_price)\n",
    "    highest_and_lowest_price.toPandas().to_csv(r\"C:\\Users\\Asad\\Documents\\Propwise Challenge\\analytics_files\\highest_and_lowest_priced_properties.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c74743cb-b164-4ecf-b9e5-b18888456886",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_data = analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70d6aa4f-ba4e-4f30-be2a-0d3462190fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d90af-8ab7-4288-8136-b9633f48d1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
